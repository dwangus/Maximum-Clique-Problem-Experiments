What did I learn from my approach in Basically5.5.txt...
I learned that it's more efficient and practical to build a solution from the ground and narrow
your search gradually that way, as there are (in densely populated graphs at least) going to be
multiple unique solutions of max clique size... and so, it makes no sense to begin narrowing down
based off which nodes don't converge as fast according to some metric, iteratively one-by-one, 
as you may be inadvertently cutting off possible solutions to the set of nodes you do leave behind.

Next round of approaches (as the last one was reasonably successful in determining node's 
"preferences" among neighbors)...
Interestingly enough, if you can, for any arbitrary graph, simply approximate the SIZE of the max
clique... you can use that very method recursively on closed neighborhoods of each node to then
find a set of nodes that fits the bill (based on which nodes end up having a POHNS of SIZE-1).
--> Approach it this way. It may be computationally more efficient, and it allows you to find 
	general solutions and heuristics that conceptually allow you to tackle bigger graphs.
--> First possible heuristic... think about how... if you have a worst case graph of c k-1 cliques,
	...the ratio of edge density... related to the number of c k-1 cliques... related to the
	overall size of the graph... and how... if you gradually begin to insert edges between nodes
	... at what point will there be a "tipping point" such that there begin to surface c-1 k-1 
	cliques and 2 k cliques, etc. OR that the new max clique size is now k+1...?
	--> Keep in mind that you should also "tailor" your approximation, based off the degree of each
		node in the graph, and particularly also, the specific other nodes of specific degree
		that each node is connected to (this is where my "preferences" ordering might come in
		handy)... BUT, do so in a way that's non-recursive (AKA it doesn't end up calling itself
		when evaluating each closed neighborhood and each closed neighborhood of each closed
		neighborhood, etc.)

--> ...alpha^k = 1/n? Probing the graph randomly x number of times, noting the average, and based off that and probability, predicting an alpha? And doing so for 
	closed neighborhoods?


#######################
My heuristic, based off test8.4_numpy.py and ripple.py -->
- Run main() in test8.4_numpy.py and obtain preference ordering among all individual nodes. --> Typically run at 100 iterations
- Using preference ordering, run a ripple-effect from ripple.py (AKA, you can choose rippling(),
	reverseRippling(), and within either, choose the factor-kernel -- AKA, linear, sqrt, etc.)
	to obtain a singular ordering. --> Typically run at 100 iterations
- Using the obtained ordering from ripple.py, select the highest-ranked node, and form a closed neighborhood 
	of that node.
- Repeat until there are no nodes left.
- Return the clique and size of the clique.

--> Implementation detail: obtain smoother, weighted orderings from test8.4_numpy.py
--> Implementation detail: your factor TIMES their factor?

--> Play around with different factor-functions, different weights, different ways of multiplying out the ripple-effect

--> Whoaaaaaaaaaaaaa....
	#Holy shit, this is so interesting...
	#If I set the update to divide by its number of neighbors after each update,
	#+ the regular factor set to value[key]
	#+ the update being your factor * their factor
	#... I get a shift of values from 10, to 50, to something pretty consistent @100+...
	#... AND, if I change the function to being factor = math.sqrt(value[key])... things change differently too!
	
	#...What if I just keep this going over time? And I keep multiplying everything by some 10^+k of the (biggest number -1)?
	#	What would happen?

	#Furthermore... what if I just... didn't have orderings? I just... from the POV of each node, gave each node an equal weighting
	#	at the beginning (based on how many nodes I had -- so 1/125 points to each of my 125 neighbors), divided (or multiplied! Idk) 
	#	each node by their number of neighbors, and their new point value was... what factor they were allowed to have. And just let it run.
	#	What would happen?

--> ###Develop more heuristics that delete edges, not nodes, as time goes on

--> Hmmmmmmmmmmmmm.... the nodes that appear most frequently at the END of a randomly found maximal set... hmmm....
#######################
#Note: Randomness is very attractive -- allows for parallelizable, concurrent computation
New heuristic, based off hello.py:
- Find edge rankings based off random probes.
- Based off edge rankings for each individual node, create new probabilities of selecting that edge when probing and finding maximal cliques.
	(AKA such that, for each node's edges to other nodes, create a probability that when selecting this node during maximal clique process,
	you create relative probabilities of which remaining neighbors to select next based on that node's individual relative predix values of such
	remaining neighbors (not of all neighbors, just remaining ones))
- Repeat
- After k iterations of this cycle, run MetaMetaRandomFind() in maximal.py, given these new edge probabilities
#######################
- Venn Diagram of mutual neighbors, but in N-dimensional space
- ...Can you do it in 3D space? How do you embed N points on the surface of a sphere such that no 4 are in the same plane? (does it relate to the Tetrahedral #?)
	--> No... I don't think so... the region of space within the sphere, if highlighted by 1 clique, is not mutually exclusive -- other cliques might just
		end up highlighting portions of the same region
- If ...given N points... you... highlight the region of space between two neighbors and all their mutually-shared neighbors... do so for each edge... and then
	define a function MC(x,y,z,...) in N-dimensional space (not N-1) that depends on |E| (for number of edges -- and therefore, number of highlighted regions of space)
	piece-wise functions (each of which specifies 1 within their highlighted region of space, 0 outside) whose height/value in the N-th dimension is dependent 
	on the values of each such piece-wise functions (dependent upon the N-1 coordinate-variables)... Then finding just one such global optimum (as there will be 
	several) in the N-1 dimensional space will specify a coordinate point which will only exist in bounded regions belonging to the specifying functions related to
	specific edges (or pairs of neighbors)...
	--> Now, of course, it's much easier to define some sort of Heaviside step-function in N-x dimensional space for each edge in E... or even some sort of...
		function that says if X + Y + Z + ... < some number (thus, creating some bounded region in higher dimensional space), f(x,y,z,...) = 1, and 0 otherwise...
		That'd be the definition of MC(x,y,z...) as an accumulation of such fE1(x,y,z...) functions... but that makes it incredibly hard to differentiate. Thus,
		it might help (for approximation reasons) to have a "falling-off" feature such that (for each fEn(x,y,z...) values smoothly (but VERY quickly) fall to 0
		once outside of the bounded region)
		--> I think... I'd have to compromise and literally cut corners with approximations such as (N-x)-spheres fit inside the (N-x)-tetrahedrons (as I can't
			think of any functions that directly allow me to specify a region of space so tightly as a triangle -- like (X+Y < c)), and once I've specified a
			center for the (N-x)-sphere and a radius, I could use such x,y,z... coordinates and specify a sigmoid function with the exponent-coefficient
			beta approaching infinity (or at least big enough that it satisfies some pre-determined epsilon based on N), and x in the exponent = (x+y+z+...
			minus (or plus, idk which right now) some c)
			--> That way, MC(x,y,z...) can just be a function of the sum of all "edge-functions" in |E|
			--> Oops, in order for it to be "bounded", it has to be c1 <= (x - ...)+(y - ...)+(z - ...)+... <= c2 ==> No, wait, it just has to be 
				0 <= (x - ...)+(y - ...)+(z - ...)+... <= (c2 = Radius)
				--> Which makes things easier, as we can specify that x,y,z... are all positive 
			--> Which also shouldn't be too difficult finding the (N-x) sphere, just calculate the center of the given points, and to find its radius, ...
				do some maths lol, I can figure this out later
			--> Yea, and then for the sigmoid function, we do theta = (x - ...)+(y - ...)+(z - ...)+..., the coefficient-beta = some ridiculously large number
				to satisfy some delta, and f(theta) = (1/(1+e^((theta + radius)*beta))) --> Positive e-exponent such that theta-values < radius register as 1
			--> And then we define MC(x,y,z...) as the sum of these f(theta) functions for each edge in |E|
			--> Oops, it should be theta = (x - ...)^2 + (y - ...)^2 + (z - ...)^2 ... - radius^2, and f(theta) = 1/(1 + e^(beta*theta))
--> How might concurrent computing help such an approach?
- But, of course, the problem still is -- how do you find the darkest/most-highlighted region of space?
	--> ...Approximation method, such as not to get stuck in one local optimum at a time. Start at the center of the total region of space from all x,y,z... variables.
		Define a new function as such: A(x,y,z...) = Continuous sum of MC(x,y,z...) as x,y,z start at center-coordinate, and dx=dy=dz=d... (basically, for the 2D
		analog, we have MC(x,y) on the z-axis, and then define A(x,y) as a function of the sum of MC(x,y) starting at the center of the bounded region of space,
		and increasing the "radius" of A(x,y) at a constant rate, as a function of x,y)... Then, we differentiate A(x,y,z...) and see where the largest "jumps"
		are -- and then using those points in N-1 dimensional space as starting points, narrow our search of the N-1 dimensional space surrounding those points
		to (hopefully) find global optimums.
--> No... something's wrong... these (N-x)-spheres exist along lower-dimensional planes than (N-1)... so... you have to implement a theta in the sigmoid function
	such that if any variables not belonging to an edge-function (AKA that an edge-function does not depend on) are varied, that edge-function's value quickly
	falls to 0... that it will have a "thin-width" w/ respect to other variables... Therefore, it stands to reason that the center of some (N-x)-sphere can only
	exist along that explicit (N-x)-plane... so which variables have to be held constant?
	--> Rephrased another way, say you have a circle existing along some tilted plane in 3D space. How do we change the theta-function to reflect that, if we
		move any x,y,z-coordinate such that we are no longer along that plane, f(theta) quickly falls to 0?
		--> Plot z = 1/(1+(e^(beta*(((x-2)^2 + (y-2)^2) - 4))))
		--> Plot y = 4*(1/(1+e^(beta*(x-3))))*(1/(1+e^(-beta*(x-3))))
		--> Plot y = 1/(1+(e^(300(x-3))))
		--> So... f(x,y,z...) should be like... with theta(x,y,dependent variables...) = (x - ...)^2 + (y - ...)^2 + ... - radius^2
			and antitheta(z,independent variables...) = (z - ...) + (a - ...) + (b - ...) + ...,
			then f(x,y,z,a,b,...) = [1/(1 + e^(beta*theta))]*[4*(1/(1 + e^(beta*antitheta)))*(1/(1 + e^(-beta*antitheta)))]... I think...
			--> The only caveat with the above equation is that we have to have each node originally be placed on separate axes... right...?
				Or does that not matter when we "zero" it out with (x - ...)? How do we determine the "independent" variables?
				--> http://math.stackexchange.com/questions/714711/how-to-find-n1-equidistant-vectors-on-an-n-sphere
				--> Well... if we specify k equidistant points in an (N-1)-dimensional space, each of those k equidistant points (if 
					we found them "the right way") will have (N-k-1) "redundant" dimensions shared amongst all of them, none of which
					need be used when forming the specified radius and center of the (k-1)-sphere. Thus, we specify the "dependent"
					variables of theta based on which constraints/dimensions the k equidistant points require, and specify the "independent"
					variables of antitheta based on which redundant dimensions (needed to specify the center of the (k-1)-sphere, but
					needed for nothing more) are leftover...? Seems reasonable...?
				--> Relevant: "Finding a sphere from 4 points", "Finding a circle from 3 points", "Finding center of triangle", "Finding center
					of tetrahedron", "Finding 3-sphere from 5 points"
					--> http://math.stackexchange.com/questions/894794/sphere-equation-given-4-points
--> So, to sum it all up:
	-- Create N equidistant points (for each node in the graph) in an (N-1)-dimensional coordinate system.
		(Note: specify a huge spacing margin for the distance between all points)
	-- For each edge in the graph, look at the endpoints' mutually shared neighbors -- based on all such k points (including endpoints), find the center & radius of the
		corresponding inscribed (k-1)-sphere, and based on what dependent and independent dimensions were found in the creation of this (k-1)-sphere, use
		the radius, center-coordinates, dependent, and independent variable-dimensions and form an equation f(theta, antitheta, radius, center). This is called an
		"edge-function". Add this to a running list of equations.
		(Note: choose a beta that's a huge enough number to satisfy some epsilon or delta, based on N, the size of the original graph)
	-- Create a new function, called MC(a,b,c,...x,y,z,...(N-1)th-variable) = sum(edge-functions).
	-- Run a non-convex maximization optimization algorithm to find the coordinates of one (of many) local optimum of the MC(...) function, in the bounded region of space
		constrainted by the N equidistant points.
		--> ...In the hopes that it finds a global optima
	-- Given the coordinates of this one global optimum, find the distance to all N equidistant points. Sort the list of respective distances, and return either: ...
		Option A) the batch of points (and respective nodes -- which all should be part of some clique) that all are within some miniscule delta of each other,
				at the very top of the list, or...
		Option B) the first point's corresponding node in the list. Repeat this process on the graph of the closed neighborhood of this node.

--> See:
	-- "motzkin and strauss max clique quadratic programming"

--> Hmm... when running a non-convex maximization optimization algorithm.... say I find a local optima of height k (as a returned value of MC(...))...
	Why can't I just... "flood" the epigraph to the base height of k and modify MC(...) thereafter? 
	As in, let MC(...) = sum(edge-functions). Find local maxima of height k after running the non-convex optimization algorithm. 
	Append local maxima coordinate to running list.
	Modify MC(...) += (-k + k*(1/(1 + e^(beta*theta)))), wherein theta is now dependent upon ALL (N-1) dimensions -- is now an added (N-2)-sphere -- 
		where theta = (x - ...)^2 + (y - ...)^2 + (z - ...)^2 + (a - ...)^2 + (b - ...)^2 + ... - radius^2.
	Repeat, with new additions of k based on new "filtered" local optima found by the non-convex maximization optimization algorithm. 
	Once the final "local" minima found is the center of the entire body of points, return the last found local optima.
	--> Does this actually work...? NO, it doesn't...

	--> How do I "flood" the pits in a smooth manner???
	--> WAIT!!! I GOT IT. 
	Let MC(...) = ...
	Let MC'(...) += k*((1/(1 + e^(beta*theta))) - 1/(1 + e^(-beta'*(MC(...) - k/2.))), wherein theta is now dependent upon ALL (N-1) dimensions 
		-- is now an added (N-2)-sphere -- where theta = (x - ...)^2 + (y - ...)^2 + (z - ...)^2 + (a - ...)^2 + (b - ...)^2 + ... - radius^2.
	All subsequent repetitions of MC''...'(...) use the original MC(...) as a nested sum-sigmoidal function.
		--> Note: The second term... varying beta' has its ups and downs (literally).
		--> Rule of thumb: solve for beta' by setting 1/(1 + e^(-beta'*(k - k/2))) = some y such that (1.0 - y) < some threshold epsilon specified
		--> Along with this, we have to vary the learning-rate alpha (or some other gradient checking when performing gradient ascent) such as to 
			skip over all local optima whose gradient/rate/derivative falls below a specific lower bound?
	--> It's not exact... but it shouldn't be too bad either...
	--> Furthermore, one could speed this algorithm up by finding the coordinates of some local optima based on the coordinates of some corresponding large clique
		found through other means -- and then "flood-zero" out the higher-dimensional epigraph of the height of the local optima of that corresponding large clique.
		(Like taking hops of 30, then 12, then 5, etc., until a global optima is found corresponding to the coordinate-center of a max-clique)
	--> And given that one could use concurrent computing to find all edge-functions at once (and thus, all terms in the summation of MC(...) simultaneously), that
		allows for further optimizations in run-time
	#####
	--> This is all dependent on, of course... 
		A) N-spheres are good approximations to the N-dimensional polygonal shapes they're intended to fit inside
		B) The maths is correct in saying that regions of space, shared by specific cliques, will be mutually-exclusively "highlighted"
		C) The sigmoid function and corresponding beta-coefficient for such regions of highlighted, higher-dimensional space falls off 
			quickly enough such that exclusive "dimensions" fall off quickly enough so as not to overlap with the regions of space occupied
			by other cliques
		D) Gradient ascent and other non-convex maximization algorithms are able to differentiate adequately on MC(...) and terminate in reasonable time
		E) The coordinate-center of such cliques will indeed be the points corresponding to local optima
		F) The "zero-out flood" technique does indeed help w/ reducing non-convex epigraphs, when applying gradient ascent/other convex maximization algorithms,
			and gradually filters out local optima of greater height -- AKA, finds the global optima of the epigraph
		G) ...yea, shit, there's a tradeoff between beta and how fast it would take gradient ascent to converge to a local optima...
	#####
	--> WAIT!!! I have a better approximation
		Old MC'(...) += factor*sin((2pi/k) * x)*sigmoid(beta, x-k), with the factor pre-computed based on the min-height of [Old MC'(...) - MC(...)]
	
	--> Wait... why can't I just do this:
		Let MC(...) = ...
		MC'(...) = (1.0 - sigmoid(beta, (MC(...))-k))*MC(...)
		...? 
		Why was I fiddling with all that other stuff, this pretty much zeroes it out anyways?
		And this way, I don't have to similarly add back in k*(1/(1 + e^(beta*theta))), I can just vary k based off of whatever new local optima k I find...
		-- ...actually, if I DO add back in k*(1/(1 + e^(beta*theta))), it'll allow gradient ascent to converge more quickly... (I'd probably need to input
		some smaller beta, so that everything's more gradually "converge-able", and that whatever miniscule differences are left from -sigmoid(beta, (MC(...))-k),
		they're vastly out-scaled by the sigmoid-function's own gradient...
		#####
		--> I think, ultimately, it looks like this:
			Let MC(...) = ...
			Then, MC'(...) = (1.0 - sigmoid(beta, MC(...)-k))*MC(...) + [k*sigmoid(-small_beta, theta)]*[sigmoid(beta, MC(...)-k)]
			Vary k as you find bigger k.
			(Note: set small_beta = 1?)
	--> Might be better to have a circumscribed N-sphere rather than an inscribed N-sphere...? Might help with gradient calculations and greater overlapping areas...?
		--> Actually... might be better to have a circumscribed N-sphere that's just BARELY smaller than the higher-dimensional manifold/shape/polygon it circumscribes -- i.e., such
			that it doesn't encapsulate the vertices of the higher-dimensional manifold/shape/polygon and such vertices lie outside of its region
#######################

Heuristics To Try:
1) Approximate size of max clique based on probabilities, edge densities, degrees, worst-case-graph comparisons, etc.
2) Ripple.py experiments -- w/ simultaneous clustering/filtering...
3) Meta-Random Find, based on changing edge-ranking probabilities, and based on random maximal-clique probes into graph w/ "last-one-left-popularity" heuristic
4) (most promising) "Iteratively-Convex" Optimizations Over Smoothly-Differentiable Nested-Summed Sigmoidal Functions Approximating Overlapping k-Dimensional-Spherical Spaces
	
	-- ...if this 4th approach is true... then P = NP... but I feel like... things are still preserved. At least w/ regards to human cognitive understanding and perception,
		that "there's no difference between recognizing a solution and finding one", and these implications in the analogies such as "there'd be no difference between
		appreciating a symphony and writing one"... there still is a difference. What I did, substituting MC(...) into itself, was nothing more than a clever mathematical "trick"...
		That, if I were to liken it to anything, (besides the difficulty in picturing the higher dimensionality of the whole approach) it'd be like the mental gap in finding 
		square roots of factors and negative numbers, necessitating the "temporary invention" of the intermediary, imaginary number i. That, there's still a mental "jump" there...
		If you were to plot out the steps in writing a symphony, in your invention of a new algorithm, in your finding a solution to a problem... there'd still be that kind of
		gap between your steps, inexplicable to anyone as to exactly HOW you came to think of this, but that you just "did" -- that's so necessary to describing inexplicable genius
		in the first place. And just because it is mathematically feasible, doesn't mean it ever accurately describes, in "real" terms, what really happened between step A and step B
		in crafting the final result. It simply is, and can be done, but we cannot explain why. And so long as it's inexplicable, but doable and "real"... then people can still make 
		their mental jumps and come up with amazing, beautiful, and similarly... "inexplicably real" things. And if my approach is mathematically sound and does indeed imply 
		what I think it implies, then... P=NP doesn't detract from that Magic which is, and always will be, "inexplicably real".

#######################
Added notes to Heuristic 4):
-- http://mathoverflow.net/questions/32533/is-all-non-convex-optimization-heuristic
-- http://www.sbras.ru/interval/Library/Thematic/GlobOptim/GlOptUsingIA-1.pdf
-- "The requirements on the function are that it be Lipschitz or smooth to some order, and that it not have a combinatorial explosion of local optima."
	--> I... think... my function is Lipschitz continuous? It's pretty much all composed of sigmoids, after all...
	--> But... the "combinatorial" explosion of local optimas is definitely a problem...
		--> But, as I see the context of the comment referring to the above paper in "Interval Analysis"... the intervals he's deleting from consideration
			are across the dimensions of the input vector of X, not across the singular dimension of f(X) -- so of course combinatorial local minima explosion
			would drastically affect the running time of his method, in terms of the number of intervals needed to be evaluated...

-- If I'm making the "slices" of dimensions incredibly thin, and that the sigmoidal function falls off really quickly for each individual edge in MC(...)... then, when I do transform the 
	graph into MC'(...), the added term of + [k*sigmoid(-small_beta, theta)]*[sigmoid(beta, MC(...)-k)] might end up, in huge dimensions, causing random starting points chosen during
	gradient ascent to just ignore (because the "thin peaks" in N-dimensional space barely factor in at all into the surrounding gradient) any other local optima and converge at the
	very center... So random walking that continuously ends up converging at the center of the body of N equidistant coordinates, for that particular local optima of the entire bounded
	region of space... is sure to break my heuristic.

-- But wait. I think can go a step further in my analysis... 
	
	Let k = some integer constant (preferably some height k = c(c-1)/2, for a discrete integer c < N). 
	
	When I transform MC(...) into MC'(...)... When I perform gradient ascent to find some local optima, the "random walking" I do, w/o loss from generalization, can be seen as a direct
	"walk" from one vertex at the absolute boundary of the bounded region of space (so, maybe coordinate (0,0,0...)) to the very center of the bounded region of space. And... if it's
	the case that the height k I've assumed IS indeed, now, the global optima of MC'(...), then, really, the gradient of my entire N-dimensional epigraph will be nearly identical to 
	the gradient of that of a regular sigmoidal function over the entire bounded region of space -- that is, for all coordinates in the space, gradient(MC'(...)) ~= gradient(k*sigmoid(-small_beta, theta)).
	Otherwise, if it's not the case, and that there is some global optima I'm missing, then of course, there exist some coordinates in the space where gradient(MC'(...)) != gradient(k*sigmoid(-small_beta, theta)).
													(!=, in this case, refers to how the absolute difference of the compared gradients > some tiny, tiny epsilon threshold)
	...But, moreover, because I'm assuming that I'm walking in ONE direction -- that is, d(MC')/dx = d(MC')/dy = d(MC')/dz = d(MC')/da = ... -- we can directly reduce the dimensionality of 
	k*sigmoid(-small_beta, theta) to 1! That is, replace all instances of y,z,a,b,c... in the function with x, and thus, now, the function becomes k*sigmoid(-small_beta, (N-2)(xi - ...)^2). But FURTHERMORE,
	it stands to reason, then, that if there exist some coordinates in the space where gradient(MC'(...)) != gradient(k*sigmoid(-small_beta, theta)), then if we similarly replace all y,z,a,b,c...
	variables in MC'(...) w/ x, then the same ***SHOULD HOLD*** (big if), that there exist some value of x in 1-dimensional space such that gradient(MC'(x)) != gradient(k*sigmoid(-small_beta, (N-2)(xi - ...)^2)).
	Otherwise stated, if there is some global optima, for a specific k, that I'm "missing" by walking straight to the center, then for some values of x (in the analogous bounded region in 1-dimension), 
	d(MC'(x))/dx != d(k*sigmoid(-small_beta, (N-2)(xi - ...)^2))/dx.
	==> Which, I think... similarly implies that integral(MC'(x)dx) > integral(k*sigmoid(-small_beta, (N-2)(xi - ...)^2)dx), for the range (presumably) [0, distance((0,0,0...), center of N-dimensional space)*2]
		(Note: It really does depend on how "thin" the slices would be transformed into 1-dimension, and if that would add any significant area to the integral of (MC'(x)dx)... but then again,
			one might be able to smoothly vary the upper bound in the range [0, ___] to be able to better detect if there are any drastic differences between the two)
		-- Otherwise stated, one can analyze [integral(MC'(x)dx) - integral(k*sigmoid(-small_beta, (N-2)(xi - ...)^2)dx)] to see if it's > 0 (or some epsilon threshold very close to 0)
	==> Meaning... I think this is implied... If you calculate out [integral(MC'(x)dx) - integral(k*sigmoid(-small_beta, (N-2)(xi - ...)^2)dx)] for incremented discrete values of c, where k = c(c-1)/2...
		... then... the point that that function (which I guess I'll call MCNSID(k,x) (max-clique-nested-sigmoidal-integral-difference)) converges to 0... 
		... might it be... the size of the max clique???********************

	If that was true... then this algorithm would run MUCH faster... i.e., it would take in total O((N^5)*log(N)) time (n^2 for "edge-function", n checks of mutual neighbors for each edge, another n^2 for multiplying
	terms out in MC'(...), and also, for varying values of c up to N (In fact, you wouldn't even have to use much of c -- you could guess a high number, or start from a lower/upper bound, and slowly increment from there, or
	just make guesses on halves of N))...
	But, also, you could drastically speed up the compute time with concurrent/parallel computing, and a powerful differential engine, probably up to O(N^3) time (let log(N) processors compute log(N) different guesses,
	and N^2 processors find the edge-functions simultaneously, pooling up the summation function of MC(...)).
	And, of course, determining an actual clique of such size simply requires repeating this entire procedure on the closed neighborhood-graphs of each individual node.
#######################


































