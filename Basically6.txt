What did I learn from my approach in Basically5.5.txt...
I learned that it's more efficient and practical to build a solution from the ground and narrow
your search gradually that way, as there are (in densely populated graphs at least) going to be
multiple unique solutions of max clique size... and so, it makes no sense to begin narrowing down
based off which nodes don't converge as fast according to some metric, iteratively one-by-one, 
as you may be inadvertently cutting off possible solutions to the set of nodes you do leave behind.

Next round of approaches (as the last one was reasonably successful in determining node's 
"preferences" among neighbors)...
Interestingly enough, if you can, for any arbitrary graph, simply approximate the SIZE of the max
clique... you can use that very method recursively on closed neighborhoods of each node to then
find a set of nodes that fits the bill (based on which nodes end up having a POHNS of SIZE-1).
--> Approach it this way. It may be computationally more efficient, and it allows you to find 
	general solutions and heuristics that conceptually allow you to tackle bigger graphs.
--> First possible heuristic... think about how... if you have a worst case graph of c k-1 cliques,
	...the ratio of edge density... related to the number of c k-1 cliques... related to the
	overall size of the graph... and how... if you gradually begin to insert edges between nodes
	... at what point will there be a "tipping point" such that there begin to surface c-1 k-1 
	cliques and 2 k cliques, etc. OR that the new max clique size is now k+1...?
	--> Keep in mind that you should also "tailor" your approximation, based off the degree of each
		node in the graph, and particularly also, the specific other nodes of specific degree
		that each node is connected to (this is where my "preferences" ordering might come in
		handy)... BUT, do so in a way that's non-recursive (AKA it doesn't end up calling itself
		when evaluating each closed neighborhood and each closed neighborhood of each closed
		neighborhood, etc.)

--> ...alpha^k = 1/n? Probing the graph randomly x number of times, noting the average, and based off that and probability, predicting an alpha? And doing so for 
	closed neighborhoods?


#######################
My heuristic, based off test8.4_numpy.py and ripple.py -->
- Run main() in test8.4_numpy.py and obtain preference ordering among all individual nodes. --> Typically run at 100 iterations
- Using preference ordering, run a ripple-effect from ripple.py (AKA, you can choose rippling(),
	reverseRippling(), and within either, choose the factor-kernel -- AKA, linear, sqrt, etc.)
	to obtain a singular ordering. --> Typically run at 100 iterations
- Using the obtained ordering from ripple.py, select the highest-ranked node, and form a closed neighborhood 
	of that node.
- Repeat until there are no nodes left.
- Return the clique and size of the clique.

--> Implementation detail: obtain smoother, weighted orderings from test8.4_numpy.py
--> Implementation detail: your factor TIMES their factor?

--> Play around with different factor-functions, different weights, different ways of multiplying out the ripple-effect

--> Whoaaaaaaaaaaaaa....
	#Holy shit, this is so interesting...
	#If I set the update to divide by its number of neighbors after each update,
	#+ the regular factor set to value[key]
	#+ the update being your factor * their factor
	#... I get a shift of values from 10, to 50, to something pretty consistent @100+...
	#... AND, if I change the function to being factor = math.sqrt(value[key])... things change differently too!
	
	#...What if I just keep this going over time? And I keep multiplying everything by some 10^+k of the (biggest number -1)?
	#	What would happen?

	#Furthermore... what if I just... didn't have orderings? I just... from the POV of each node, gave each node an equal weighting
	#	at the beginning (based on how many nodes I had -- so 1/125 points to each of my 125 neighbors), divided (or multiplied! Idk) 
	#	each node by their number of neighbors, and their new point value was... what factor they were allowed to have. And just let it run.
	#	What would happen?

--> ###Develop more heuristics that delete edges, not nodes, as time goes on

--> Hmmmmmmmmmmmmm.... the nodes that appear most frequently at the END of a randomly found maximal set... hmmm....
#######################
#Note: Randomness is very attractive -- allows for parallelizable, concurrent computation
New heuristic, based off hello.py:
- Find edge rankings based off random probes.
- Based off edge rankings for each individual node, create new probabilities of selecting that edge when probing and finding maximal cliques.
	(AKA such that, for each node's edges to other nodes, create a probability that when selecting this node during maximal clique process,
	you create relative probabilities of which remaining neighbors to select next based on that node's individual relative predix values of such
	remaining neighbors (not of all neighbors, just remaining ones))
- Repeat
- After k iterations of this cycle, run MetaMetaRandomFind() in maximal.py, given these new edge probabilities
#######################
- Venn Diagram of mutual neighbors, but in N-dimensional space
- ...Can you do it in 3D space? How do you embed N points on the surface of a sphere such that no 4 are in the same plane? (does it relate to the Tetrahedral #?)
	--> No... I don't think so... the region of space within the sphere, if highlighted by 1 clique, is not mutually exclusive -- other cliques might just
		end up highlighting portions of the same region
- If ...given N points... you... highlight the region of space between two neighbors and all their mutually-shared neighbors... do so for each edge... and then
	define a function MC(x,y,z,...) in N-dimensional space (not N-1) that depends on |E| (for number of edges -- and therefore, number of highlighted regions of space)
	piece-wise functions (each of which specifies 1 within their highlighted region of space, 0 outside) whose height/value in the N-th dimension is dependent 
	on the values of each such piece-wise functions (dependent upon the N-1 coordinate-variables)... Then finding just one such global optimum (as there will be 
	several) in the N-1 dimensional space will specify a coordinate point which will only exist in bounded regions belonging to the specifying functions related to
	specific edges (or pairs of neighbors)...
	--> Now, of course, it's much easier to define some sort of Heaviside step-function in N-x dimensional space for each edge in E... or even some sort of...
		function that says if X + Y + Z + ... < some number (thus, creating some bounded region in higher dimensional space), f(x,y,z,...) = 1, and 0 otherwise...
		That'd be the definition of MC(x,y,z...) as an accumulation of such fE1(x,y,z...) functions... but that makes it incredibly hard to differentiate. Thus,
		it might help (for approximation reasons) to have a "falling-off" feature such that (for each fEn(x,y,z...) values smoothly (but VERY quickly) fall to 0
		once outside of the bounded region)
		--> I think... I'd have to compromise and literally cut corners with approximations such as (N-x)-spheres fit inside the (N-x)-tetrahedrons (as I can't
			think of any functions that directly allow me to specify a region of space so tightly as a triangle -- like (X+Y < c)), and once I've specified a
			center for the (N-x)-sphere and a radius, I could use such x,y,z... coordinates and specify a sigmoid function with the exponent-coefficient
			beta approaching infinity (or at least big enough that it satisfies some pre-determined epsilon based on N), and x in the exponent = (x+y+z+...
			minus (or plus, idk which right now) some c)
			--> That way, MC(x,y,z...) can just be a function of the sum of all "edge-functions" in |E|
			--> Oops, in order for it to be "bounded", it has to be c1 <= (x - ...)+(y - ...)+(z - ...)+... <= c2 ==> No, wait, it just has to be 
				0 <= (x - ...)+(y - ...)+(z - ...)+... <= (c2 = Radius)
				--> Which makes things easier, as we can specify that x,y,z... are all positive 
			--> Which also shouldn't be too difficult finding the (N-x) sphere, just calculate the center of the given points, and to find its radius, ...
				do some maths lol, I can figure this out later
			--> Yea, and then for the sigmoid function, we do theta = (x - ...)+(y - ...)+(z - ...)+..., the coefficient-beta = some ridiculously large number
				to satisfy some delta, and f(theta) = (1/(1+e^((theta + radius)*beta))) --> Positive e-exponent such that theta-values < radius register as 1
			--> And then we define MC(x,y,z...) as the sum of these f(theta) functions for each edge in |E|
			--> Oops, it should be theta = (x - ...)^2 + (y - ...)^2 + (z - ...)^2 ... - radius^2, and f(theta) = 1/(1 + e^(beta*theta))
--> How might concurrent computing help such an approach?
- But, of course, the problem still is -- how do you find the darkest/most-highlighted region of space?
	--> ...Approximation method, such as not to get stuck in one local optimum at a time. Start at the center of the total region of space from all x,y,z... variables.
		Define a new function as such: A(x,y,z...) = Continuous sum of MC(x,y,z...) as x,y,z start at center-coordinate, and dx=dy=dz=d... (basically, for the 2D
		analog, we have MC(x,y) on the z-axis, and then define A(x,y) as a function of the sum of MC(x,y) starting at the center of the bounded region of space,
		and increasing the "radius" of A(x,y) at a constant rate, as a function of x,y)... Then, we differentiate A(x,y,z...) and see where the largest "jumps"
		are -- and then using those points in N-1 dimensional space as starting points, narrow our search of the N-1 dimensional space surrounding those points
		to (hopefully) find global optimums.
--> No... something's wrong... these (N-x)-spheres exist along lower-dimensional planes than (N-1)... so... you have to implement a theta in the sigmoid function
	such that if any variables not belonging to an edge-function (AKA that an edge-function does not depend on) are varied, that edge-function's value quickly
	falls to 0... that it will have a "thin-width" w/ respect to other variables... Therefore, it stands to reason that the center of some (N-x)-sphere can only
	exist along that explicit (N-x)-plane... so which variables have to be held constant?
	--> Rephrased another way, say you have a circle existing along some tilted plane in 3D space. How do we change the theta-function to reflect that, if we
		move any x,y,z-coordinate such that we are no longer along that plane, f(theta) quickly falls to 0?
		--> Plot z = 1/(1+(e^(beta*(((x-2)^2 + (y-2)^2) - 4))))
		--> Plot y = 4*(1/(1+e^(beta*(x-3))))*(1/(1+e^(-beta*(x-3))))
		--> Plot y = 1/(1+(e^(300(x-3))))
		--> So... f(x,y,z...) should be like... with theta(x,y,dependent variables...) = (x - ...)^2 + (y - ...)^2 + ... - radius^2
			and antitheta(z,independent variables...) = (z - ...) + (a - ...) + (b - ...) + ...,
			then f(x,y,z,a,b,...) = [1/(1 + e^(beta*theta))]*[4*(1/(1 + e^(beta*antitheta)))*(1/(1 + e^(-beta*antitheta)))]... I think...
			--> The only caveat with the above equation is that we have to have each node originally be placed on separate axes... right...?
				Or does that not matter when we "zero" it out with (x - ...)? How do we determine the "independent" variables?
				--> http://math.stackexchange.com/questions/714711/how-to-find-n1-equidistant-vectors-on-an-n-sphere
				--> Well... if we specify k equidistant points in an (N-1)-dimensional space, each of those k equidistant points (if 
					we found them "the right way") will have (N-k-1) "redundant" dimensions shared amongst all of them, none of which
					need be used when forming the specified radius and center of the (k-1)-sphere. Thus, we specify the "dependent"
					variables of theta based on which constraints/dimensions the k equidistant points require, and specify the "independent"
					variables of antitheta based on which redundant dimensions (needed to specify the center of the (k-1)-sphere, but
					needed for nothing more) are leftover...? Seems reasonable...?
				--> Relevant: "Finding a sphere from 4 points", "Finding a circle from 3 points", "Finding center of triangle", "Finding center
					of tetrahedron", "Finding 3-sphere from 5 points"
					--> http://math.stackexchange.com/questions/894794/sphere-equation-given-4-points
--> So, to sum it all up:
	-- Create N equidistant points (for each node in the graph) in an (N-1)-dimensional coordinate system.
		(Note: specify a huge spacing margin for the distance between all points)
	-- For each edge in the graph, look at the endpoints' mutually shared neighbors -- based on all such k points (including endpoints), find the center & radius of the
		corresponding inscribed (k-1)-sphere, and based on what dependent and independent dimensions were found in the creation of this (k-1)-sphere, use
		the radius, center-coordinates, dependent, and independent variable-dimensions and form an equation f(theta, antitheta, radius, center). This is called an
		"edge-function". Add this to a running list of equations.
		(Note: choose a beta that's a huge enough number to satisfy some epsilon or delta, based on N, the size of the original graph)
	-- Create a new function, called MC(a,b,c,...x,y,z,...(N-1)th-variable) = sum(edge-functions).
	-- Run a non-convex maximization optimization algorithm to find the coordinates of one (of many) local optimum of the MC(...) function, in the bounded region of space
		constrainted by the N equidistant points.
		--> ...In the hopes that it finds a global optima
	-- Given the coordinates of this one global optimum, find the distance to all N equidistant points. Sort the list of respective distances, and return either: ...
		Option A) the batch of points (and respective nodes -- which all should be part of some clique) that all are within some miniscule delta of each other,
				at the very top of the list, or...
		Option B) the first point's corresponding node in the list. Repeat this process on the graph of the closed neighborhood of this node.

--> See:
	-- "motzkin and strauss max clique quadratic programming"

--> Hmm... when running a non-convex maximization optimization algorithm.... say I find a local optima of height k (as a returned value of MC(...))...
	Why can't I just... "flood" the epigraph to the base height of k and modify MC(...) thereafter? 
	As in, let MC(...) = sum(edge-functions). Find local maxima of height k after running the non-convex optimization algorithm. 
	Append local maxima coordinate to running list.
	Modify MC(...) += (-k + k*(1/(1 + e^(beta*theta)))), wherein theta is now dependent upon ALL (N-1) dimensions -- is now an added (N-2)-sphere -- 
		where theta = (x - ...)^2 + (y - ...)^2 + (z - ...)^2 + (a - ...)^2 + (b - ...)^2 + ... - radius^2.
	Repeat, with new additions of k based on new "filtered" local optima found by the non-convex maximization optimization algorithm. 
	Once the final "local" minima found is the center of the entire body of points, return the last found local optima.
	--> Does this actually work...? NO, it doesn't...

	--> How do I "flood" the pits in a smooth manner???
	--> WAIT!!! I GOT IT. 
	Let MC(...) = ...
	Let MC'(...) += k*((1/(1 + e^(beta*theta))) - 1/(1 + e^(-beta'*(MC(...) - k/2.))), wherein theta is now dependent upon ALL (N-1) dimensions 
		-- is now an added (N-2)-sphere -- where theta = (x - ...)^2 + (y - ...)^2 + (z - ...)^2 + (a - ...)^2 + (b - ...)^2 + ... - radius^2.
	All subsequent repetitions of MC''...'(...) use the original MC(...) as a nested sum-sigmoidal function.
		--> Note: The second term... varying beta' has its ups and downs (literally).
		--> Rule of thumb: solve for beta' by setting 1/(1 + e^(-beta'*(k - k/2))) = some y such that (1.0 - y) < some threshold epsilon specified
		--> Along with this, we have to vary the learning-rate alpha (or some other gradient checking when performing gradient ascent) such as to 
			skip over all local optima whose gradient/rate/derivative falls below a specific lower bound?
	--> It's not exact... but it shouldn't be too bad either...
	--> Furthermore, one could speed this algorithm up by finding the coordinates of some local optima based on the coordinates of some corresponding large clique
		found through other means -- and then "flood-zero" out the higher-dimensional epigraph of the height of the local optima of that corresponding large clique.
		(Like taking hops of 30, then 12, then 5, etc., until a global optima is found corresponding to the coordinate-center of a max-clique)
	--> And given that one could use concurrent computing to find all edge-functions at once (and thus, all terms in the summation of MC(...) simultaneously), that
		allows for further optimizations in run-time
	#####
	--> This is all dependent on, of course... 
		A) N-spheres are good approximations to the N-dimensional polygonal shapes they're intended to fit inside
		B) The maths is correct in saying that regions of space, shared by specific cliques, will be mutually-exclusively "highlighted"
		C) The sigmoid function and corresponding beta-coefficient for such regions of highlighted, higher-dimensional space falls off 
			quickly enough such that exclusive "dimensions" fall off quickly enough so as not to overlap with the regions of space occupied
			by other cliques
		D) Gradient ascent and other non-convex maximization algorithms are able to differentiate adequately on MC(...) and terminate in reasonable time
		E) The coordinate-center of such cliques will indeed be the points corresponding to local optima
		F) The "zero-out flood" technique does indeed help w/ reducing non-convex epigraphs, when applying gradient ascent/other convex maximization algorithms,
			and gradually filters out local optima of greater height -- AKA, finds the global optima of the epigraph
		G) ...yea, shit, there's a tradeoff between beta and how fast it would take gradient ascent to converge to a local optima...
	#####
	--> WAIT!!! I have a better approximation
		Old MC'(...) += factor*sin((2pi/k) * x)*sigmoid(beta, x-k), with the factor pre-computed based on the min-height of [Old MC'(...) - MC(...)]
	
	--> Wait... why can't I just do this:
		Let MC(...) = ...
		MC'(...) = (1.0 - sigmoid(beta, (MC(...))-k-1))*MC(...)
		...? 
		Why was I fiddling with all that other stuff, this pretty much zeroes it out anyways?
		And this way, I don't have to similarly add back in k*(1/(1 + e^(beta*theta))), I can just vary k based off of whatever new local optima k I find...
		-- ...actually, if I DO add back in k*(1/(1 + e^(beta*theta))), it'll allow gradient ascent to converge more quickly... (I'd probably need to input
		some smaller beta, so that everything's more gradually "converge-able", and that whatever miniscule differences are left from -sigmoid(beta, (MC(...))-k-1),
		they're vastly out-scaled by the sigmoid-function's own gradient...
		#####
		--> I think, ultimately, it looks like this:
			Let MC(...) = ...
			Then, MC'(...) = (1.0 - sigmoid(beta, MC(...)-k-1))*MC(...) + [k*sigmoid(-small_beta, theta)]*[sigmoid(beta, MC(...)-k-1)]
			Vary k as you find bigger k.
			(Note: set small_beta = 1?)
	--> Might be better to have a circumscribed N-sphere rather than an inscribed N-sphere...? Might help with gradient calculations and greater overlapping areas...?
		--> Actually... might be better to have a circumscribed N-sphere that's just BARELY smaller than the higher-dimensional manifold/shape/polygon it circumscribes -- i.e., such
			that it doesn't encapsulate the vertices of the higher-dimensional manifold/shape/polygon and such vertices lie outside of its region
#######################

Heuristics To Try:
1) Approximate size of max clique based on probabilities, edge densities, degrees, worst-case-graph comparisons, etc.
2) Ripple.py experiments -- w/ simultaneous clustering/filtering...
3) Meta-Random Find, based on changing edge-ranking probabilities, and based on random maximal-clique probes into graph w/ "last-one-left-popularity" heuristic
4) (most promising) "Iteratively-Convex" Optimizations Over Smoothly-Differentiable Nested-Summed Sigmoidal Functions Approximating Overlapping k-Dimensional-Spherical Spaces
	
	-- ...if this 4th approach is true... then P = NP... but I feel like... things are still preserved. At least w/ regards to human cognitive understanding and perception,
		that "there's no difference between recognizing a solution and finding one", and these implications in the analogies such as "there'd be no difference between
		appreciating a symphony and writing one"... there still is a difference. What I did, substituting MC(...) into itself, was nothing more than a clever mathematical "trick"...
		That, if I were to liken it to anything, (besides the difficulty in picturing the higher dimensionality of the whole approach) it'd be like the mental gap in finding 
		square roots of factors and negative numbers, necessitating the "temporary invention" of the intermediary, imaginary number i. That, there's still a mental "jump" there...
		If you were to plot out the steps in writing a symphony, in your invention of a new algorithm, in your finding a solution to a problem... there'd still be that kind of
		gap between your steps, inexplicable to anyone as to exactly HOW you came to think of this, but that you just "did" -- that's so necessary to describing inexplicable genius
		in the first place. And just because it is mathematically feasible, doesn't mean it ever accurately describes, in "real" terms, what really happened between step A and step B
		in crafting the final result. It simply is, and can be done, but we cannot explain why. And so long as it's inexplicable, but doable and "real"... then people can still make 
		their mental jumps and come up with amazing, beautiful, and similarly... "inexplicably real" things. And if my approach is mathematically sound and does indeed imply 
		what I think it implies, then... P=NP doesn't detract from that Magic which is, and always will be, "inexplicably real".

#######################
Added notes to Heuristic 4):
-- http://mathoverflow.net/questions/32533/is-all-non-convex-optimization-heuristic
-- http://www.sbras.ru/interval/Library/Thematic/GlobOptim/GlOptUsingIA-1.pdf
-- "The requirements on the function are that it be Lipschitz or smooth to some order, and that it not have a combinatorial explosion of local optima."
	--> I... think... my function is Lipschitz continuous? It's pretty much all composed of sigmoids, after all...
	--> But... the "combinatorial" explosion of local optimas is definitely a problem...
		--> But, as I see the context of the comment referring to the above paper in "Interval Analysis"... the intervals he's deleting from consideration
			are across the dimensions of the input vector of X, not across the singular dimension of f(X) -- so of course combinatorial local minima explosion
			would drastically affect the running time of his method, in terms of the number of intervals needed to be evaluated...

-- If I'm making the "slices" of dimensions incredibly thin, and that the sigmoidal function falls off really quickly for each individual edge in MC(...)... then, when I do transform the 
	graph into MC'(...), the added term of + [k*sigmoid(-small_beta, theta)]*[sigmoid(beta, MC(...)-k)] might end up, in huge dimensions, causing random starting points chosen during
	gradient ascent to just ignore (because the "thin peaks" in N-dimensional space barely factor in at all into the surrounding gradient) any other local optima and converge at the
	very center... So random walking that continuously ends up converging at the center of the body of N equidistant coordinates, for that particular local optima of the entire bounded
	region of space... is sure to break my heuristic.

-- But wait. I think can go a step further in my analysis... 
	
	Let k = some integer constant (preferably some height k = c(c-1)/2, for a discrete integer c < N). 
	
	When I transform MC(...) into MC'(...)... When I perform gradient ascent to find some local optima, the "random walking" I do, w/o loss from generalization, can be seen as a direct
	"walk" from one vertex at the absolute boundary of the bounded region of space (so, maybe coordinate (0,0,0...)) to the very center of the bounded region of space. And... if it's
	the case that the height k I've assumed IS indeed, now, the global optima of MC'(...), then, really, the gradient of my entire N-dimensional epigraph will be nearly identical to 
	the gradient of that of a regular sigmoidal function over the entire bounded region of space -- that is, for all coordinates in the space, gradient(MC'(...)) ~= gradient(k*sigmoid(-small_beta, theta)).
	Otherwise, if it's not the case, and that there is some global optima I'm missing, then of course, there exist some coordinates in the space where gradient(MC'(...)) != gradient(k*sigmoid(-small_beta, theta)).
													(!=, in this case, refers to how the absolute difference of the compared gradients > some tiny, tiny epsilon threshold)
	...But, moreover, because I'm assuming that I'm walking in ONE direction -- that is, d(MC')/dx = d(MC')/dy = d(MC')/dz = d(MC')/da = ... -- we can directly reduce the dimensionality of 
	k*sigmoid(-small_beta, theta) to 1! That is, replace all instances of y,z,a,b,c... in the function with x, and thus, now, the function becomes k*sigmoid(-small_beta, (N-2)(xi - ...)^2). But FURTHERMORE,
	it stands to reason, then, that if there exist some coordinates in the space where gradient(MC'(...)) != gradient(k*sigmoid(-small_beta, theta)), then if we similarly replace all y,z,a,b,c...
	variables in MC'(...) w/ x, then the same ***SHOULD HOLD*** (big if), that there exist some value of x in 1-dimensional space such that gradient(MC'(x)) != gradient(k*sigmoid(-small_beta, (N-2)(xi - ...)^2)).
	Otherwise stated, if there is some global optima, for a specific k, that I'm "missing" by walking straight to the center, then for some values of x (in the analogous bounded region in 1-dimension), 
	d(MC'(x))/dx != d(k*sigmoid(-small_beta, (N-2)(xi - ...)^2))/dx.
	==> Which, I think... similarly implies that integral(MC'(x)dx) > integral(k*sigmoid(-small_beta, (N-2)(xi - ...)^2)dx), for the range (presumably) [0, distance((0,0,0...), center of N-dimensional space)*2]
		(Note: It really does depend on how "thin" the slices would be transformed into 1-dimension, and if that would add any significant area to the integral of (MC'(x)dx)... but then again,
			one might be able to smoothly vary the upper bound in the range [0, ___] to be able to better detect if there are any drastic differences between the two)
		-- Otherwise stated, one can analyze [integral(MC'(x)dx) - integral(k*sigmoid(-small_beta, (N-2)(xi - ...)^2)dx)] to see if it's > 0 (or some epsilon threshold very close to 0)
	==> Meaning... I think this is implied... If you calculate out [integral(MC'(x)dx) - integral(k*sigmoid(-small_beta, (N-2)(xi - ...)^2)dx)] for incremented discrete values of c, where k = c(c-1)/2...
		... then... the point that that function (which I guess I'll call MCNSID(k,x) (max-clique-nested-sigmoidal-integral-difference)) converges to 0... 
		... might it be... the size of the max clique???********************

	If that was true... then this algorithm would run MUCH faster... i.e., it would take in total O((N^5)*log(N)) time (n^2 for "edge-function", n checks of mutual neighbors for each edge, another n^2 for multiplying
	terms out in MC'(...), and also, for varying values of c up to N (In fact, you wouldn't even have to use much of c -- you could guess a high number, or start from a lower/upper bound, and slowly increment from there, or
	just make guesses on halves of N))...
	But, also, you could drastically speed up the compute time with concurrent/parallel computing, and a powerful differential engine, probably up to O(N^3) time (let log(N) processors compute log(N) different guesses,
	and N^2 processors find the edge-functions simultaneously, pooling up the summation function of MC(...)).
	And, of course, determining an actual clique of such size simply requires repeating this entire procedure on the closed neighborhood-graphs of each individual node.

	--> Ah, wait, shit, one HUGE caveat: because of a large beta in antitheta, the "added" slice is going to be super thin, and might not add much to the integral at all (or at least enough to be
		detected)... I guess, then, you'd just have to compare the derivatives of both functions, and see which ones would drastically change, given tiny steps (the number of steps would be 
		basically the width of the entire bounded region divided by a detectable width of a sigmoid function's derivative "peak" given a beta... which, unfortunately, multiplies my entire
		algorithm out by a variable factor of time... yikes.)
		--> (Eh... then again... if beta is 1000... the integral of that "fall-off" for ONE sigmoid function, w/ measured-peak-width of 0.006... is actually 0.0013 --
		 that's not HORRIBLE to detect...)
			--> But it might be w/ respect to some large height of k...

--> To sum up, for each edge, we have: 
	f(x,y,z,a,b,...) = [1/(1 + e^(beta1*theta))]*[4*(1/(1 + e^(beta2*antitheta)))*(1/(1 + e^(-beta2*antitheta)))],
		where 	theta(x,y,dependent variables...) = (x - ...)^2 + (y - ...)^2 + ... - radius^2
			where 	radius = some 0.99... fraction of the distance between the circumscribed (n-1)-sphere of the subset-body of n points,
			and	(x - ...)^2 + (y - ...)^2 + ... refers to the dependent (non-redundant) variables of center of lower-dimensional (n-1)-sphere of subset-body of n points
		and   	antitheta(z,independent variables...) = (z - ...) + (a - ...) + (b - ...) + ...
			where (z - ...) + (a - ...) + (b - ...) + ... refers to the independent (redundant) variables that the lower-dimensional "plane" of the subset-body of n points that
				houses the lower-dimensional (n-1)-sphere need not be concerned with
	
	MC(...) = sum(f(theta, antitheta) functions for each edge),
	MC'(...) = (1.0 - sigmoid(beta3, MC(...)-k-1))*MC(...) + [k*sigmoid(-beta4, theta2)]*[sigmoid(beta3, MC(...)-k-1)],
		where 	theta2(x,y,z,a,b,c...) = (x - ...)^2 + (y - ...)^2 + (z - ...)^2 + (a - ...)^2 + ... - radius^2
			where radius = some 0.99... fraction of the distance between the circumscribed (N-1)-sphere of the body of N points
	/// (confirmed, the below does not work)
	and, after replacing all variables in MC(...) w/ one variable x,
	MCNSID = [integral(MC'(x)dx) - integral(k*sigmoid(-beta4, (N-2)(xi - ...)^2)dx)]
	and, new function NSDD (nested sigmoidal derivative difference),
	NSDD = [d(MC'(x))/dx - d(k*sigmoid(-beta4, (N-2)(xi - ...)^2))/dx]
--> Note: seems to be best if I vary the beta2 w/ antitheta to be ...medium?, the beta1 w/ theta torc be ...medium?, the beta3 in the outer sigmoid to be large, and the beta4 in the added "flood term" of MC'(...)
	to be ~1.
	--> Because, in reality, the "peaks" being detectable in the 2D graph vary largely on beta1 and beta2... as in, a large beta3 ensures that, for all values of x in the
		2D graph, such values of x that achieve a height k in MC'(...) get 0'd out very quickly, and setting beta4 to 1 allows for a "normal frame of reference",
		but beta1 specifies how quickly terms in MC(...) fall to 0 w/ values of x outside of a solvable range, and beta2 specifies how quickly terms in MC(...) fall to
		0 outside of one particular value of x...
		--> And furthermore, ... if theta(x) = 0 --> meaning, one can solve for (x - ...)^2 + (y - ...)^2 + ... <= radius^2... presumably solving for 2 points of x,
				a lower bound (L(x)) and an upper bound (U(x)), and you have a range of values in that range which x can occupy...
				and, setting antitheta = 0, meaning (x' - ...) + (x' - ...) + ... = 0, one can solve for x'... w/ 1 value...
				...What happens if x' is not between L(x) and U(x)??? Wouldn't that mean that the "peak" wouldn't register 
				in the 2D graph, as outside of [L(x),U(x)], values quickly drop to 0, and outside of x', values quickly drop to 0??? 
				Can I prove that that would never happen?
				--> Maybe not necessarily... as solving for x' in this case essentially just finds the average of all redundant coordinates in specifying the
					lower-dimensional plane of that particular lower-dimensional (n-1)-sphere...
				--> No, you know what would make more sense? Instead of changing the individual edge-functions of form f(x,y,z,a,b) to f(x) by just replacing
					all instances of variables w/ x... you instead change it to:
					f(x) = [1/(1 + e^(beta1*theta))]*[sum of ([4*(1/(1 + e^(beta2*(x - c1))))*(1/(1 + e^(-beta2*(x - c1))))]) for each unique c term in antitheta]
					--> But how does that change the original transformation of MC'(...)? ...Well, that actually does complicate things. Yea, for sure, it changes
					the achievable height of k...
					--> Fuck, the original way actually changes things too. If it's just finding the average of all redundant coordinates, then you might not
					necessarily have overlaps of edge-function terms corresponding to the same values of x' (AKA, regions of mutually-shared space between edges)... 
					And, likewise, you could have edge-function terms sharing corresponding values of x' that DON'T share such regions in higher dimensions... SHIT!
						--> Well, it might be interesting to see that... all combinations of cliques' centers might occupy different (by INCREDIBLY slight degrees)
							discrete points in higher-dimensional space, such that the average of the redundant coordinates for each center might never actually
							occupy the same point in 1-dimension... but that's conjecture. And even if that were true, you'd basically have to search for "spike-peaks"
							in the 2D graph (by having an EXTREMELY large beta2), of taking steps of practically 0-dimensions, that approach infinity, 
							exploring all possible, continuous values of x... shit.
							--> ****...If the conjecture were true, could you find differences in NSDD analytically? AKA, solve for values of x where NSDD > 1?****

--> What if... instead of transforming MC'(...) into one dimension for MCNSID and NSDD... I instead just integrated over all dimensions to find the N-dimensional-manifold-surface-area and compared
	the values? Two notes immediately come to mind: this approach would actually work better if I varied beta2 to be smaller (as lower-dimensional "widths" that are narrow would barely contribute
	to the overall value of the integral over all dimensions). More importantly... the second note: it might not be possible to integrate (confirmed). Nesting sigmoidal functions within each other... 
	might be impossible to integrate as a result. (Buuut, I mean... it might be possible to approximate?) But the fact that as c varies, k = c*(c-1)/2 varies a lot faster helps "counter" the "thin-ness" of
	lower-dimensional planes...?
	--> Wait... if I have MC'(...) and (k*sigmoid(-beta4, theta2) = ) Reference-Sigma(k), if I compare integrations across each INDIVIDUAL dimension... if any of them are different, then we know
		that there exists other, local optima I'm not considering... right? (Well, the "thin-ness" of lower-dimensional planes still exists as a problem...)
	--> Curse of dimensionality!!!
	--> Mmmm... no, wait, if MC'(...) = (1.0 - sigmoid(beta3, MC(...)-k-1))*MC(...) + [k*sigmoid(-beta4, theta2)]*[sigmoid(beta3, MC(...)-k-1)], then it's a lot easier to just have 
		antiMC'(...) = (1.0 - sigmoid(beta3, MC(...)-k-1))*MC(...) -- and then integrate over antiMC'(...) over all dimensions for varying values of k (k w.r.t. c), and make an INCREDIBLY large
		beta3, a "modest" beta1 and beta2 (but not too modest, definitely still needs to be steep, steep enough that "combinatorial explosion" summations over tiny, tiny values in the fringes
		of the sigmoid function don't actually end up counting for anything), and see when c eventually (but definitively) varies to 0 (when beta3 approaches infinity)... Does that work?
		--> Ah... hahahaha... fuck. Integrating antiMC'(...) over N dimensions... leads to 2^N terms to evaluate, eventually... Wow, shit.
			--> http://link.springer.com/chapter/10.1007%2F978-94-009-3889-2_23
--> ...Can you use interval analysis in conjunction with my usage of "zero-then-flood" technique? I.e., bound the range of values considered (like, guess an upper bound to the size of the max clique,
	and then only search for local optima within the "intermediate shell"/intermediate space between the original (N-1)-sphere and the (U(N)-1)-sphere? ...But, I mean, that still doesn't prevent 
	you from finding local optima at the fringes of the inner-sphere...

-- https://homes.cs.washington.edu/~afriesen/papers/nips2013_opt.pdf
	--> This honestly seems so damn similar/close to what I was doing... and missing my core aspects by millimeters... And it was done in 2013 -- which leads me to believe that my idea/approach
	might perhaps be new...?
	--> Found by googling "eliminating local optima combinatorial non-convex optimization"

-- https://en.wikipedia.org/wiki/Duality_%28optimization%29
-- https://inst.eecs.berkeley.edu/~ee127a/book/login/l_dual_weak_lag.html
	--> These, too, seem to be strongly, intrinsically tied to what I'm attempting...

-- Somehow substituting MC'(...) into itself...? Creating a new dimension that varies with time? What??? (If I evaluate one point, I CAN'T MOVE FROM THAT POINT -- i.e., I can't use the information at that
	point to influence the values at other points)

-- Last, real, approach is to solve analytically for x (if it's even possible)

-- Last note before I go back to studying for finals at 2:50am lol:
	--> MC'(...) modified = (1.0 - sigmoid(beta3, MC(...)-k-1))*MC(...)
	--> After each iteration, wherein I find local optima of greater and greater height k, I make beta3 sharper, and I relax beta1 and beta2 more (because at least, if I relax beta1 and beta2
		gradually after greater k are found, I'm less likely to have orders-of-magnitude-of local optima "collapse" and have sum of their intermittent "fall-off" values actually reach this
		newer height k (which we vary w/ c, actually, not k)
		--> And, of course, interval analysis is always a thing
		--> + Numerical approximations for integration (since you can't actually integrate, analytically, a nested sigmoidal function -- like even sigmoid(1, x^2 + y^2) is unapproachable)
		--> Also, it might help to consider having the n-spheres be bigger than the in-sphere, and smaller than the circum-sphere... or wait, smaller than the in-sphere? Idk, I'd have to figure this out
#######################
#######################
Added notes p.2 to Heuristic 4):
-- Ah shit, specifying a k-sphere in higher dimensions w/ redundant "variables" requires parameterizing everything... which is a huge pain in the ass. (AKA a circle in 3D requires the center, the radius, and
	enough vectors (so in this case, 2) perpendicular to each other to create a unique span/plane of that dimension, and then writing the N variables of the N-dimensional space in terms of k variables...)
	...So, what I'm instead going to do is have a redundant dimension that simplifies the math a bit. So, N nodes begets N dimensions to specify each node's coordinates in N-dimensional space. If the
	equidistant spacing between nodes is supposed to be s, then each node-coordinate will be something like <...0,0, s/sqrt(2), 0,0,...>
	--> Ah... shit, that'll make the above conjecture clearly false though...
-- http://www.math.tamu.edu/~yvorobet/MATH304-503/Lect3-07web.pdf --> slide 10
-- Antitheta is the distance from a coordinate point to the bounded region of subspace spanned by the lower-dimensional k-sphere (can be obtained through Gram-Schmidt process) (NOT the entire subspace itself!!!)
	--> Need to research "distance from point to span of vectors", "distance from point to n-sphere", "how to describe span of vectors", "equation of sphere in 4d", "how to determine if two spans intersect"
-- Also notes to self: for theta in the edge-function... apparently we need it to be the circum-sphere's radius? Otherwise, for some collapsed, 1-dimensional parabolas, there exists no solution to f(x)...
	Which kind of makes sense, as a mid-sphere or in-sphere might be excluding regions of higher-dimensional space when collapsed to 1-dimension. Still though, do we have to worry about, with circum-spheres,
	"exclusive" regions of space belonging to mutual groups of neighbors...? (And by the way, as each term in MC(...) can only be 1, and there are only so many edges, the value of MC(...) is bounded by the
	number of edges in the original graph, so as long as we stay well below that threshold with a large beta3 (and I guess betas 1 and 2), we should be fine about any "combinatorial explosions")

-- So, we've been able to confirm that (at least with a circum-sphere radius), that differing thetas for differing edge-functions will always pretty much have differing ranges of x in the collapsed 1-dimensional
	case... but we've yet to confirm that for antitheta (which is actually the more important of the two to confirm -- theta bounds it to a range, but antitheta bounds it to pretty much a single value (also, we
	have yet to confirm that the value antitheta yields will always FALL within the range of x-values given by theta))... nor have we been able to definitively confirm that different groups of nodes 
	(as determined from mutual neighbors for each edge-function) and their k-spheres occupy mutually exclusive regions of space, except for overlapping between shared mutual neighbors...
	--> To take the simplest example of mutually exclusive regions of space, imagine edges AB, AC, AD, AE, BC, BD, BE, CE -- (AB) will contribute a sum of 1 to the group ABCDE, (AC, AE, BC, BE, CE) will 
		contribute a sum of 5 to the group ABCE, and (AD, BD) will contribute a sum of 2 to the group ABD... Now, does regions ABCE and ABD simply share the edge AB?
		--> Furthermore, we DON'T want their sharing of edge AB to show... which is why a bit smaller than an in-sphere was proposed... but we don't necessarily have a choice if we collapse it to 1-dimension.
			--> You could always... bound the region of searched space (at least in higher dimensions -- the reverse doesn't hold in 1 dimension)? AKA, not include the edges of the entire region of space...
				And you could go a step further, only searching towards the inner part of the region of space, to exclude lower cliques...? ...Is this going to be a problem?
				--> ...Am I confused or something? I keep flopping back and forth... I now think that the radius definitely HAS to be larger than the in-sphere's radius (else there'd be no overlap in the
					first place...)...
	--> Remember, you can just blindly use Gram-Schmidt Process to find the distance from a coordinate point to a lower-dimensional subspace... it "zeroes" everything out in the process, so if you did collapse it
		into 1-dimension, you're left with cx, where c is some coefficient -- you can't solve for x because the center of the subspace (i.e. the center of the k-sphere) was "zeroed" out in order to make the 
		calculations of the distance more feasible... gotta find a way to preserve this...
#######################
#######################
Added notes p.3 to Heuristic 4):
-- Ah, ok, I finally found the way to make exclusive regions of space. (And I see no reason why it shouldn't work with N dimensions vs. N-1 dimensions, to make the calculations easier)
	--> You have to VARY THE RADIUS of the k-sphere based off what size-of-max-clique c you're trying to guess. Take, for example, a sphere whose center coincides with a tetrahedron's in 3D space. If you
		vary the radius of the sphere to be 0, it's a point -- and it will ONLY touch the center of the tetrahedron. If you vary the radius to be an in-sphere, then it touches the centers of the
		4 triangle-faces of the tetrahedron -- it's touching the center of PLANES of 2D. If you vary the radius to be a mid-sphere, then it's touching the midpoints of all EDGES of the tetrahedron
		-- AKA, it's touching the center of LINES of 1D. And finally, if you vary the radius to be a circum-sphere, then it's touching all vertices of the tetrahedron, all of 0D.
	--> Thus, you have your same old equations, with f(beta1,beta2,x,y,z,w,v,a,b,c...) as an edge-function, of MC(...) = sum(f(...)), of MC'(beta3,...) = (1 - sigmoid(beta3, MC(...) - k-1))*(MC(...)),
		except, this time, within f(...), while theta and antitheta are held constant (and in N dimensions, it's much easier to compute antitheta and theta), the radius^2 is varied accordingly:
		--> radius = distance(center of group of mutually-shared nodes from viewpoint of specific edge, center of random sub-group of size-c of group of mutually-shared nodes)
		--> And furthermore, if the guess size c is the same as the size of the group of mutually-shared nodes, then the radius is 0; otherwise, if it's smaller, then the radius is negative infinity
-- ...This still doesn't help me figure out what to do with a "zeroed-out" graph of higher-dimensions... 
#######################
#######################
-- Local Optima Integers Gradient Ascent
-- How to find an adequate starting point on the lower bound once you've zeroed out the space
-- Uniform sphere approximation?
-- Start from upper bound shell from center (once you have an estimate)

-- RandomFind + Merging Maximal Independent Sets!!! Should drastically decrease upper bound
	--> It doesn't... and I think I FINALLY understand why it doesn't. When I merge a max independent carrying a Max-Clique-Candidate,
		what I can end up doing is merging a maximally independent set such that this first MCC is now connected to another MCC (and perhaps
		multiple other MCC's) that it wasn't connected to before... meaning that whatever max clique the other MCC was connected to, now 
		has just increased by 1 in size.
	--> Not to mention, that my reasoning is fundamentally flawed... 

-- Coupling maximal independent sets (negative weights) with a variety of approaches??? I.e. vector-projection method, negative regions in Heuristic 4), etc.
#######################
#######################
Added notes p.4 to Heuristic 4):
-- ...What if I... so you have MC'(...), of N variables. What if... I created a new function, MC''(...). And this is what it was:
	--> For every possible pair of N variables (so x-y, y-z, x-z, etc.) we create a modified version of MC'(...). So if
		f(x,y,z) was (x-3)^2 + (y-2)^2 + z, then the modified f(y,x,z) would be (y-3)^2 + (x-2)^2 + z.
	--> We then define MC''(...) to be the sum of all versions of MC'(...). 
		...Because... unless the max clique lies at the center of the entire region of space, at the center of the N-sphere...
		implying that the max clique size is of N itself... every region in space that coincides with a +1 point from each
		term in MC(...) must have some axis of symmetry wherein the position it's reflected does not stay still.
	--> What does this mean, and why does it make things feasible? ...We couldn't, before, reduce things down to one dimension.
		Meaning, we couldn't cast a shadow against some specific axis (rendering other variables pointless in MC(...)), as
		the point defined in the higher-N-dimensional plane achieved some of its "k-value" depending on one or more variables
		that became redundant in the very process of dimensional-reduction. Meaning, that, once we reduced it to a specific axis,
		some terms that may have been able to exist above the set k-threshold in a higher dimensional space (if we evaluated the
		function at that specific point with multiple coordinates) would lose some of its k-value and be "zeroed-out" in the process --
		and therefore, once we cast the "axis-shadow", we wouldn't know it was there. It would "fly under the radar" and be undetected.
		Thus, if we looked at it vs. one specific dimension, we wouldn't be able to tell by the axis-shadow if there existed some point
		in the higher-dimensional space that achieved a clique of greater size than what we were predicting -- AKA, if there was a point,
		in a simple y-v.-x 2D graph, where x was > some value of k for y.
	--> HOWEVER... If we create this new function MC''(...), as a sum of N^2 MC'(...) terms, rotated across all possible axes... what happens
		is that, if we are to look at a certain dimension (and ONLY that dimension) -- for example, we choose to ignore all terms except
		for x in MC''(...) (so ultimately, change all N-sphere equations to equations of x-parabolas (radius held constant), and ignore all terms in antitheta that do not
		possess x -- if there are none, those 2 functions reduce to evaluating to 1, always) (not that we set all other variables EQUAL to X in the function... 
		there's a subtle difference) -- then, at some point, we'll have swapped out the "lost" k-value from another variable with x in a 
		linearly separable (as it's still a sum of terms, each individual term still zeroing-out if the original term falls beneath a set-certain k-threshold -- 
		and that's beautiful) fashion. Meaning, while other terms above the threshold might get a "boost" and appear WAY over the threshold (up
		to N^2 times higher than it originally was), all other terms that would have been "undetected"... finally do appear. And it doesn't
		matter that other points in the x-dimension get evaluated way higher -- all we care about are whether there EXISTS points above
		the threshold or not. So if we get to see ALL of the ones above the threshold, even if some of their values are now over-exaggerated...
		It still answers the question. Does there exist a point in this higher-dimensional space with a value of k higher than the one we set?
		If there are none... the no. Else, yes.
	--> Of course... the biggest caveat is that, even if this is bounded... what, we have N^2 functions for each edge going into MC(...)...
		then we have nested sigmoidal functions of MC(...) multiplied as another factor to MC(...) for MC'(...)... so that's already N^4 individual 
		terms... and THEN, we have N^2 VARIATIONS of MC'(...) all summed together to create MC''(...)... AND THEN, we have to, for values 1 < c <= N,
		check all variations of k = c(c-1)/2, in one dimension, to see if there's a point above the k-threshold we're setting. 
		All in all... that's O(N^6) steps for the clique-decision problem, O(N^7) steps for the max-clique problem... O(N^8) if we want to find actual
		members of the max clique, by looking at the closed neighborhoods of each individual node...
		Which... all in all, is realistically infeasible.

			But it'd be bounded.
-- Lol, jk, 6AM rambling with no sleep -- my entire thing is flawed. Lulz.
#######################
#######################





























