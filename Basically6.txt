What did I learn from my approach in Basically5.5.txt...
I learned that it's more efficient and practical to build a solution from the ground and narrow
your search gradually that way, as there are (in densely populated graphs at least) going to be
multiple unique solutions of max clique size... and so, it makes no sense to begin narrowing down
based off which nodes don't converge as fast according to some metric, iteratively one-by-one, 
as you may be inadvertently cutting off possible solutions to the set of nodes you do leave behind.

Next round of approaches (as the last one was reasonably successful in determining node's 
"preferences" among neighbors)...
Interestingly enough, if you can, for any arbitrary graph, simply approximate the SIZE of the max
clique... you can use that very method recursively on closed neighborhoods of each node to then
find a set of nodes that fits the bill (based on which nodes end up having a POHNS of SIZE-1).
--> Approach it this way. It may be computationally more efficient, and it allows you to find 
	general solutions and heuristics that conceptually allow you to tackle bigger graphs.
--> First possible heuristic... think about how... if you have a worst case graph of c k-1 cliques,
	...the ratio of edge density... related to the number of c k-1 cliques... related to the
	overall size of the graph... and how... if you gradually begin to insert edges between nodes
	... at what point will there be a "tipping point" such that there begin to surface c-1 k-1 
	cliques and 2 k cliques, etc. OR that the new max clique size is now k+1...?
	--> Keep in mind that you should also "tailor" your approximation, based off the degree of each
		node in the graph, and particularly also, the specific other nodes of specific degree
		that each node is connected to (this is where my "preferences" ordering might come in
		handy)... BUT, do so in a way that's non-recursive (AKA it doesn't end up calling itself
		when evaluating each closed neighborhood and each closed neighborhood of each closed
		neighborhood, etc.)

--> ...alpha^k = 1/n? Probing the graph randomly x number of times, noting the average, and based off that and probability, predicting an alpha? And doing so for 
	closed neighborhoods?


#######################
My heuristic, based off test8.4_numpy.py and ripple.py -->
- Run main() in test8.4_numpy.py and obtain preference ordering among all individual nodes. --> Typically run at 100 iterations
- Using preference ordering, run a ripple-effect from ripple.py (AKA, you can choose rippling(),
	reverseRippling(), and within either, choose the factor-kernel -- AKA, linear, sqrt, etc.)
	to obtain a singular ordering. --> Typically run at 100 iterations
- Using the obtained ordering from ripple.py, select the highest-ranked node, and form a closed neighborhood 
	of that node.
- Repeat until there are no nodes left.
- Return the clique and size of the clique.

--> Implementation detail: obtain smoother, weighted orderings from test8.4_numpy.py
--> Implementation detail: your factor TIMES their factor?

--> Play around with different factor-functions, different weights, different ways of multiplying out the ripple-effect

--> Whoaaaaaaaaaaaaa....
	#Holy shit, this is so interesting...
	#If I set the update to divide by its number of neighbors after each update,
	#+ the regular factor set to value[key]
	#+ the update being your factor * their factor
	#... I get a shift of values from 10, to 50, to something pretty consistent @100+...
	#... AND, if I change the function to being factor = math.sqrt(value[key])... things change differently too!
	
	#...What if I just keep this going over time? And I keep multiplying everything by some 10^+k of the (biggest number -1)?
	#	What would happen?

	#Furthermore... what if I just... didn't have orderings? I just... from the POV of each node, gave each node an equal weighting
	#	at the beginning (based on how many nodes I had -- so 1/125 points to each of my 125 neighbors), divided (or multiplied! Idk) 
	#	each node by their number of neighbors, and their new point value was... what factor they were allowed to have. And just let it run.
	#	What would happen?

--> ###Develop more heuristics that delete edges, not nodes, as time goes on

--> Hmmmmmmmmmmmmm.... the nodes that appear most frequently at the END of a randomly found maximal set... hmmm....
#######################
#Note: Randomness is very attractive -- allows for parallelizable, concurrent computation
New heuristic, based off hello.py:
- Find edge rankings based off random probes.
- Based off edge rankings for each individual node, create new probabilities of selecting that edge when probing and finding maximal cliques.
	(AKA such that, for each node's edges to other nodes, create a probability that when selecting this node during maximal clique process,
	you create relative probabilities of which remaining neighbors to select next based on that node's individual relative predix values of such
	remaining neighbors (not of all neighbors, just remaining ones))
- Repeat
- After k iterations of this cycle, run MetaMetaRandomFind() in maximal.py, given these new edge probabilities
#######################
- Venn Diagram of mutual neighbors, but in N-dimensional space
- ...Can you do it in 3D space? How do you embed N points on the surface of a sphere such that no 4 are in the same plane? (does it relate to the Tetrahedral #?)
	--> No... I don't think so... the region of space within the sphere, if highlighted by 1 clique, is not mutually exclusive -- other cliques might just
		end up highlighting portions of the same region
- If ...given N points... you... highlight the region of space between two neighbors and all their mutually-shared neighbors... do so for each edge... and then
	define a function MC(x,y,z,...) in N-dimensional space (not N-1) that depends on |E| (for number of edges -- and therefore, number of highlighted regions of space)
	piece-wise functions (each of which specifies 1 within their highlighted region of space, 0 outside) whose height/value in the N-th dimension is dependent 
	on the values of each such piece-wise functions (dependent upon the N-1 coordinate-variables)... Then finding just one such global optimum (as there will be 
	several) in the N-1 dimensional space will specify a coordinate point which will only exist in bounded regions belonging to the specifying functions related to
	specific edges (or pairs of neighbors)...
	--> Now, of course, it's much easier to define some sort of Heaviside step-function in N-x dimensional space for each edge in E... or even some sort of...
		function that says if X + Y + Z + ... < some number (thus, creating some bounded region in higher dimensional space), f(x,y,z,...) = 1, and 0 otherwise...
		That'd be the definition of MC(x,y,z...) as an accumulation of such fE1(x,y,z...) functions... but that makes it incredibly hard to differentiate. Thus,
		it might help (for approximation reasons) to have a "falling-off" feature such that (for each fEn(x,y,z...) values smoothly (but VERY quickly) fall to 0
		once outside of the bounded region)
		--> I think... I'd have to compromise and literally cut corners with approximations such as (N-x)-spheres fit inside the (N-x)-tetrahedrons (as I can't
			think of any functions that directly allow me to specify a region of space so tightly as a triangle -- like (X+Y < c)), and once I've specified a
			center for the (N-x)-sphere and a radius, I could use such x,y,z... coordinates and specify a sigmoid function with the exponent-coefficient
			beta approaching infinity (or at least big enough that it satisfies some pre-determined epsilon based on N), and x in the exponent = (x+y+z+...
			minus (or plus, idk which right now) some c)
			--> That way, MC(x,y,z...) can just be a function of the sum of all "edge-functions" in |E|
			--> Oops, in order for it to be "bounded", it has to be c1 <= (x - ...)+(y - ...)+(z - ...)+... <= c2 ==> No, wait, it just has to be 
				0 <= (x - ...)+(y - ...)+(z - ...)+... <= (c2 = Radius)
				--> Which makes things easier, as we can specify that x,y,z... are all positive 
			--> Which also shouldn't be too difficult finding the (N-x) sphere, just calculate the center of the given points, and to find its radius, ...
				do some maths lol, I can figure this out later
			--> Yea, and then for the sigmoid function, we do theta = (x - ...)+(y - ...)+(z - ...)+..., the coefficient-beta = some ridiculously large number
				to satisfy some delta, and f(theta) = (1/(1+e^((theta + radius)*beta))) --> Positive e-exponent such that theta-values < radius register as 1
			--> And then we define MC(x,y,z...) as the sum of these f(theta) functions for each edge in |E|
			--> Oops, it should be theta = (x - ...)^2 + (y - ...)^2 + (z - ...)^2 ... - radius^2, and f(theta) = 1/(1 + e^(beta*theta))
--> How might concurrent computing help such an approach?
- But, of course, the problem still is -- how do you find the darkest/most-highlighted region of space?
	--> ...Approximation method, such as not to get stuck in one local optimum at a time. Start at the center of the total region of space from all x,y,z... variables.
		Define a new function as such: A(x,y,z...) = Continuous sum of MC(x,y,z...) as x,y,z start at center-coordinate, and dx=dy=dz=d... (basically, for the 2D
		analog, we have MC(x,y) on the z-axis, and then define A(x,y) as a function of the sum of MC(x,y) starting at the center of the bounded region of space,
		and increasing the "radius" of A(x,y) at a constant rate, as a function of x,y)... Then, we differentiate A(x,y,z...) and see where the largest "jumps"
		are -- and then using those points in N-1 dimensional space as starting points, narrow our search of the N-1 dimensional space surrounding those points
		to (hopefully) find global optimums.
--> No... something's wrong... these (N-x)-spheres exist along lower-dimensional planes than (N-1)... so... you have to implement a theta in the sigmoid function
	such that if any variables not belonging to an edge-function (AKA that an edge-function does not depend on) are varied, that edge-function's value quickly
	falls to 0... that it will have a "thin-width" w/ respect to other variables... Therefore, it stands to reason that the center of some (N-x)-sphere can only
	exist along that explicit (N-x)-plane... so which variables have to be held constant?
	--> Rephrased another way, say you have a circle existing along some tilted plane in 3D space. How do we change the theta-function to reflect that, if we
		move any x,y,z-coordinate such that we are no longer along that plane, f(theta) quickly falls to 0?
		--> Plot z = 1/(1+(e^(beta*(((x-2)^2 + (y-2)^2) - 4))))
		--> Plot y = 4*(1/(1+e^(beta*(x-3))))*(1/(1+e^(-beta*(x-3))))
		--> Plot y = 1/(1+(e^(300(x-3))))
		--> So... f(x,y,z...) should be like... with theta(x,y,dependent variables...) = (x - ...)^2 + (y - ...)^2 + ... - radius^2
			and antitheta(z,independent variables...) = (z - ...) + (a - ...) + (b - ...) + ...,
			then f(x,y,z,a,b,...) = [1/(1 + e^(beta*theta))]*[4*(1/(1 + e^(beta*antitheta)))*(1/(1 + e^(-beta*antitheta)))]... I think...
			--> The only caveat with the above equation is that we have to have each node originally be placed on separate axes... right...?
				Or does that not matter when we "zero" it out with (x - ...)? How do we determine the "independent" variables?
				--> http://math.stackexchange.com/questions/714711/how-to-find-n1-equidistant-vectors-on-an-n-sphere
				--> Well... if we specify k equidistant points in an (N-1)-dimensional space, each of those k equidistant points (if 
					we found them "the right way") will have (N-k-1) "redundant" dimensions shared amongst all of them, none of which
					need be used when forming the specified radius and center of the (k-1)-sphere. Thus, we specify the "dependent"
					variables of theta based on which constraints/dimensions the k equidistant points require, and specify the "independent"
					variables of antitheta based on which redundant dimensions (needed to specify the center of the (k-1)-sphere, but
					needed for nothing more) are leftover...? Seems reasonable...?
				--> Relevant: "Finding a sphere from 4 points", "Finding a circle from 3 points", "Finding center of triangle", "Finding center
					of tetrahedron", "Finding 3-sphere from 5 points"
					--> http://math.stackexchange.com/questions/894794/sphere-equation-given-4-points
--> So, to sum it all up:
	-- Create N equidistant points (for each node in the graph) in an (N-1)-dimensional coordinate system.
		(Note: specify a huge spacing margin for the distance between all points)
	-- For each edge in the graph, look at the endpoints' mutually shared neighbors -- based on all such k points (including endpoints), find the center & radius of the
		corresponding inscribed (k-1)-sphere, and based on what dependent and independent dimensions were found in the creation of this (k-1)-sphere, use
		the radius, center-coordinates, dependent, and independent variable-dimensions and form an equation f(theta, antitheta, radius, center). This is called an
		"edge-function". Add this to a running list of equations.
		(Note: choose a beta that's a huge enough number to satisfy some epsilon or delta, based on N, the size of the original graph)
	-- Create a new function, called MC(a,b,c,...x,y,z,...(N-1)th-variable) = sum(edge-functions).
	-- Run a non-convex maximization optimization algorithm to find the coordinates of one (of many) local optimum of the MC(...) function, in the bounded region of space
		constrainted by the N equidistant points.
		--> ...In the hopes that it finds a global optima
	-- Given the coordinates of this one global optimum, find the distance to all N equidistant points. Sort the list of respective distances, and return either: ...
		Option A) the batch of points (and respective nodes -- which all should be part of some clique) that all are within some miniscule delta of each other,
				at the very top of the list, or...
		Option B) the first point's corresponding node in the list. Repeat this process on the graph of the closed neighborhood of this node.

--> See:
	-- "motzkin and strauss max clique quadratic programming"

--> Hmm... when running a non-convex maximization optimization algorithm.... say I find a local optima of height k (as a returned value of MC(...))...
	Why can't I just... "flood" the epigraph to the base height of k and modify MC(...) thereafter? 
	As in, let MC(...) = sum(edge-functions). Find local maxima of height k after running the non-convex optimization algorithm. 
	Append local maxima coordinate to running list.
	Modify MC(...) += (-k + k*(1/(1 + e^(beta*theta)))), wherein theta is now dependent upon ALL (N-1) dimensions -- is now an added (N-2)-sphere -- 
		where theta = (x - ...)^2 + (y - ...)^2 + (z - ...)^2 + (a - ...)^2 + (b - ...)^2 + ... - radius^2.
	Repeat, with new additions of k based on new "filtered" local optima found by the non-convex maximization optimization algorithm. 
	Once the final "local" minima found is the center of the entire body of points, return the last found local optima.
	--> Does this actually work...? NO, it doesn't...

	--> How do I "flood" the pits in a smooth manner???
	--> WAIT!!! I GOT IT. 
	Let MC(...) = ...
	Let MC'(...) += k*((1/(1 + e^(beta*theta))) - 1/(1 + e^(-beta'*(MC(...) - k/2.))), wherein theta is now dependent upon ALL (N-1) dimensions 
		-- is now an added (N-2)-sphere -- where theta = (x - ...)^2 + (y - ...)^2 + (z - ...)^2 + (a - ...)^2 + (b - ...)^2 + ... - radius^2.
	All subsequent repetitions of MC''...'(...) use the original MC(...) as a nested sum-sigmoidal function.
		--> Note: The second term... varying beta' has its ups and downs (literally).
		--> Rule of thumb: solve for beta' by setting 1/(1 + e^(-beta'*(k - k/2))) = some y such that (1.0 - y) < some threshold epsilon specified
		--> Along with this, we have to vary the learning-rate alpha (or some other gradient checking when performing gradient ascent) such as to 
			skip over all local optima whose gradient/rate/derivative falls below a specific lower bound?
	--> It's not exact... but it shouldn't be too bad either...
	--> Furthermore, one could speed this algorithm up by finding the coordinates of some local optima based on the coordinates of some corresponding large clique
		found through other means -- and then "flood-zero" out the higher-dimensional epigraph of the height of the local optima of that corresponding large clique.
		(Like taking hops of 30, then 12, then 5, etc., until a global optima is found corresponding to the coordinate-center of a max-clique)
	--> And given that one could use concurrent computing to find all edge-functions at once (and thus, all terms in the summation of MC(...) simultaneously), that
		allows for further optimizations in run-time
	#####
	--> This is all dependent on, of course... 
		A) N-spheres are good approximations to the N-dimensional polygonal shapes they're intended to fit inside
		B) The maths is correct in saying that regions of space, shared by specific cliques, will be mutually-exclusively "highlighted"
		C) The sigmoid function and corresponding beta-coefficient for such regions of highlighted, higher-dimensional space falls off 
			quickly enough such that exclusive "dimensions" fall off quickly enough so as not to overlap with the regions of space occupied
			by other cliques
		D) Gradient ascent and other non-convex maximization algorithms are able to differentiate adequately on MC(...) and terminate in reasonable time
		E) The coordinate-center of such cliques will indeed be the points corresponding to local optima
		F) The "zero-out flood" technique does indeed help w/ reducing non-convex epigraphs, when applying gradient ascent/other convex maximization algorithms,
			and gradually filters out local optima of greater height -- AKA, finds the global optima of the epigraph
		G) ...yea, shit, there's a tradeoff between beta and how fast it would take gradient ascent to converge to a local optima...
	#####
	--> WAIT!!! I have a better approximation
		Old MC'(...) += factor*sin((2pi/k) * x)*sigmoid(beta, x-k), with the factor pre-computed based on the min-height of [Old MC'(...) - MC(...)]
	
	--> Wait... why can't I just do this:
		Let MC(...) = ...
		MC'(...) = (1.0 - sigmoid(beta, (MC(...))-k))*MC(...)
		...? 
		Why was I fiddling with all that other stuff, this pretty much zeroes it out anyways?
		And this way, I don't have to similarly add back in k*(1/(1 + e^(beta*theta))), I can just vary k based off of whatever new local optima k I find...
		-- ...actually, if I DO add back in k*(1/(1 + e^(beta*theta))), it'll allow gradient ascent to converge more quickly... (I'd probably need to input
		some smaller beta, so that everything's more gradually "converge-able", and that whatever miniscule differences are left from -sigmoid(beta, (MC(...))-k),
		they're vastly out-scaled by the sigmoid-function's own gradient...
		#####
		--> I think, ultimately, it looks like this:
			Let MC(...) = ...
			MC'(...) = (1.0 - sigmoid(beta, MC(...)-k))*MC(...) + [k*sigmoid(-small_beta, theta)]*[sigmoid(beta, MC(...)-k)]
			Vary k as you find bigger k.
			(Note: set small_beta = 1?)
#######################














